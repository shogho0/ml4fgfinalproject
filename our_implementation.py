# -*- coding: utf-8 -*-
"""Our Implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_nYinWaW5Uk2Prbz5diTdeOtkGG_vXqu

# ML4FG Final Report: Modeling Colorectal Cancer Gene Expression Distributions using Mixture Models

Authors: Shomik Ghose (sg3789@columbia.edu), Austin Tao (alt2177@columbia.edu)
"""

#Import necessary packages
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from scipy.stats import norm

from google.colab import drive
drive.mount('/content/drive')

#Importing dataset
df = pd.read_csv("drive/MyDrive/ML4FG Final Project/Bodmer_microarray_phenotype.csv",index_col=0)
df.head()

"""## Data Preprocessing"""

df.shape

"""**Handling Duplicates**"""

#There are 33625 duplicates
df.index.duplicated().sum()

#Relabeling duplicates
df.index = df.index + ("_"+df.groupby(level=0).cumcount().astype(str)).replace('_0','')
df.head()

#Removed all duplicates
df.index.duplicated().sum()

df.loc[["CDH1","CDH1_1"],:]

"""**Handling Null Values**

We chose to keep nan values because if we turn nan values into 0, that will skew the distribution (since we want the nan value to be omitted from the analysis, not to beincluded as 0 since 0 has real meaning in this case).
"""

#Isolating a row of the data to run the model on
temp = df.loc['CDH1',:].dropna()
#Reshaping the input array to make it two-dimensional
inp = np.array(temp).reshape(-1, 1)

"""## Gaussian Mixture Model using Expectation-Maximization Algorithm"""

#Functions to determine our model selection/goodness of fit metrics

#Writing a function to identify which indices in a sorted list a number lies between
#Parameters are "lis" (a sorted list from least to greatest) and "num" (a number of the user's choosing)
#The function returns the index of the maximum number in the list that is less than "num"
def find_ind(num, lis):
  i = 0
  while num > lis[i+1]:
    i += 1
  return i

#Writing a function to identify the area under a curve using the rectangular area approximation
#Parameters are "xl" and "yl" (the x and y values of the curve)
def rec_area_under(xl,yl):
  aund = 0
  for i in range(len(xl)-1):
    aund += (yl[i]*(xl[i+1]-xl[i]))
  return aund

#Writing a function to calculate our "Adjusted Least Squares" (ALS) metric
#Parameters are "bins" (the y-values of the bins in the histogram) and "preds" (the y-values (likelihoods) of the curve fitted to the distribution) 
ind_test = []
def ls_metric(preds, bins):
  ls_ls = 0
  for i in range(len(x)):
    ind = find_ind(x[i],bins)
    ind_test.append(ind)
    ls_ls += (abs(counts[ind]-preds[i])**2)
  return ls_ls

#Writing a function to calculate our "Area Under Difference" (AUD) metric
#Parameters are "hx" and "hy" (the x and y values of the bins in the histogram) and "px" and "py" (the x and y values of the curve fitted to the distribution)
def ar_metric(hx,hy,px,py):
  hund = rec_area_under(hx,hy)
  pund = rec_area_under(px,py)
  return pund-hund

# Gaussian EM algorithm
def em_fit(X, num_components, epochs, show_plots):
  """Function to perform EM algorithm for Gaussian Mixture Model

  :Inputs:
   - *X* (vector) Input data
   - *num_components* (positive int) Desired number of clusters
   - *epochs* (positive int) Desired number of iterations to perform
   - *show_plots* (boolean) Show or don't show plots

  :Outputs:
   - *gaussians* Fitted gaussians
   - *lls* Log-likelihoods
   - *bic* Bayesian Information Criterion 
   - *scores[-1][0]+scores[-1][1]* Likelihood scores corresponding to each sample

  """

  gaussians = []
  denoms = np.zeros(num_components)
  gaussians = [norm(loc=9,scale=5),norm(loc=-1,scale=2)]
  for i in range(num_components):
    #gaussians.append(norm(loc=np.random.randint(-2,2),scale=np.random.randint(0,2))) #Instantiate Gaussians
    #gaussians.append(norm(loc=np.random.randint(5,10),scale=np.random.randint(1,2)))
    denoms[i] = 1/num_components
  
  denoms = denoms/np.sum(denoms)
  X_rs = np.reshape(X,(X.shape[0],))

  scores = []
  lls = []

  for epoch in range(epochs):
    
    if show_plots:
      cols = ['r','g','b']
      fig, ax = plt.subplots()
      counts, bins, bars = ax.hist(X_rs,weights=np.ones(len(X_rs)) / len(X_rs),ec="black")
      for i in range(num_components):
        x = X_rs
        y = gaussians[i].pdf(X)
        y = y*max(counts)/max(y)
        x,y = zip(*sorted(zip(x,y),key=lambda x: x[0]))
        ax.plot(x,y,"-k",c=cols[i])
      plt.show()


    gamma = np.zeros((X.shape[0],num_components))

    #E-step
    for i in range(num_components):
      gamma[:,i] = denoms[i]*gaussians[i].pdf(X_rs)
  
    #Row normalizing gamma matrix
    tt = np.sum(gamma,axis=1)
    for i in range(num_components):
      gamma[i] /= (np.sum(denoms)*tt[i])

    #M-step

    #Updating weight of each cluster
    fracs=[]
    for i in range(num_components):
      fracs.append(np.sum(gamma[:,i]))

    #Updating class distribution of each cluster
    pi_k=[]
    for i in range(num_components):
      pi_k.append(fracs[i]/np.sum(fracs))
    
    #Updating means of each cluster
    mu_k = np.sum(gamma*X,axis=0)/fracs

    #Updating variance of each cluster
    var_k=[]
    for i in range(num_components):
      tmp = (1/fracs[i])*np.dot((np.reshape(np.array(gamma[:,i]),(X.shape[0],1))*(X-mu_k[i])).T,(X-mu_k[i]))
      var_k.append(tmp[0,0])

    #Updating Gaussians with new mean and variance
    for i in range(num_components):
      print(mu_k[i],var_k[i])
      gaussians[i] = norm(loc=mu_k[i],scale=var_k[i])

    #Calculating likelihoods (on both per-sample and full-dataset level) for each epoch
    ll_temp = []
    for i in range(num_components):
      ll_temp.append(fracs[i]*gaussians[i].pdf(X_rs))
    scores.append(ll_temp)
    lls.append(np.log(np.sum(ll_temp)))

  #Calculating BIC
  #Gaussian mixture model has 2 parameters (mean, variance)
  bic = -2*lls[-1]+np.log(X.shape[0])*2 

  #Plotting the fit for each epoch
  if show_plots:
    cols = ['r','g','b']
    fig, ax = plt.subplots()
    counts, bins, bars = ax.hist(X_rs,weights=np.ones(len(X_rs)) / len(X_rs),ec="black")
    for i in range(num_components):
      x = X_rs
      y = gaussians[i].pdf(X)
      y = y*max(counts)/max(y) #This line normalizes the y-values of the curve so we can compare the histogram and fitted curve on the same scale
      x,y = zip(*sorted(zip(x,y),key=lambda x: x[0]))
      ax.plot(x,y,"-k",c=cols[i])
    plt.show()
  
  return gaussians, lls, bic, scores[-1][0]+scores[-1][1]

"""### Plotting Results of Gaussian Mixture Model"""

gmfit, ll1, bicg, ss = em_fit(inp,2,2,True)
print(bicg)

plt.plot(range(2),ll1)

fig, ax = plt.subplots()
counts, bins, bars = ax.hist(temp,weights=np.ones(len(temp)) / len(temp),ec="black")
x = temp
y = ss
y = y*max(counts)/max(y)
x,y = zip(*sorted(zip(x,y),key=lambda x: x[0]))
plt.title("CDH1")
plt.xlabel("log2 (expression value)")
plt.ylabel("Fraction of Samples")
print("BIC: " + str(bicg))
print("ALS: " + str(ls_metric(y,bins)))
print("AUD: " + str(ar_metric(bins,counts,x,y)))
ax.plot(x,y,"-k")

"""### Log-Likelihood Function

"""

def log_likelihood(X, num_components, n_epochs):
  log_likelihoods = []

  def get_log_likelihood(X, num_components):
    likelihood = 0
    totals = np.zeros((X.shape[0], 1), dtype=np.float64)
    gamma = np.zeros((X.shape[0], num_components), dtype=np.float64)
    denoms = np.array([1/num_components for i in range(num_components)])
    denoms = denoms/np.sum(denoms)
    gaussians = []
    
    for i in range(num_components):
      gaussians.append(norm(loc=np.random.randint(6,10),scale=np.random.randint(1,2)))
    for i in range(num_components):
      gamma[:,i] = denoms[i]*gaussians[i].pdf(np.reshape(X,(X.shape[0],)))
    
    totals = np.sum(gamma, 1)
    likelihood = np.log(totals)
    return np.sum(likelihood)

  for i in range(n_epochs):
    log_likelihood = get_log_likelihood(X, num_components)
    log_likelihoods.append(log_likelihood)
    print("Epoch: ", i + 1, "Likelihood: ", log_likelihood)

  # Plotting
  plt.figure(figsize=(10, 10))
  plt.title("Log-Likelihood")
  plt.plot(np.arange(1, n_epochs + 1), log_likelihoods)
  plt.show()

n_epochs = 100
n_components = 2

#log_likelihood(inp, n_components, n_epochs)

"""## Shifted Asymmetric Laplace (SAL) Mixture Model using EM"""

from scipy.special import kn # modified bessel function of the second kind of integer order n
from scipy.stats import expon, multivariate_normal, laplace_asymmetric 
from scipy.spatial import distance

def em_lap(X, num_components, num_epochs, show_plots):
  """Perform the EM-algorithm on the SAL mixture model

  :Inputs:
   - *X* (vector) Observed data
   - *num_components* (int) The number of clusters/components
   - *num_epochs* (int) Number of epochs to perform
   - *show_plots* (boolean) Show or don't show plots

  :Outputs:
   - 
  """

  # Initialize SAL distributions
  laps = []
  denoms = np.zeros(num_components)
  for i in range(num_components):
    laps.append(laplace_asymmetric(kappa=np.random.randint(0,5),loc=np.random.randint(8,10),scale=np.random.randint(1,2)))
    denoms[i] = 1/num_components
  
  denoms = denoms/np.sum(denoms)

  gamma = np.zeros((X.shape[0],num_components))

  # Iterate over number of epochs
  for epoch in range(num_epochs):
    
    # =========== Plotting ================
    X_rs = np.reshape(X,(X.shape[0],))

    if show_plots:
      cols = ['r','g','b']
      fig, ax = plt.subplots()
      counts, bins, bars = ax.hist(X_rs, weights=np.ones(len(X_rs)) / len(X_rs), ec="black")
      for i in range(num_components):
        x = X_rs
        y = laps[i].pdf(X)
        y = y*max(counts)/max(y)
        x,y = zip(*sorted(zip(x,y),key=lambda x: x[0]))
        ax.plot(x,y,"-k",c=cols[i])
        ax.set_xlabel("Histogram Bins for Data")
        ax.set_ylabel("Proportion of Frequencies")
        ax.set_title("Fitting Data with Asymmetric Laplace Distributions")
      
      plt.show()

    for i in range(num_components):
      laps[i] = laplace_asymmetric(kappa=np.random.randint(0,5),loc=np.random.randint(8,10),scale=np.random.randint(1,2))

num_components = 3
num_epochs = 10
em_lap(inp, num_components, num_epochs, show_plots=True)

def xi(p, X, a, cov, mu):
  """Calculate density of p-dimensional SAL distribution

  :Input:
   - *p* (int) Dimensionality of SAL distribution
   - *x* (p-dimensional vector) 
   - *a* (p-dimensional vector) skewness
   - *cov* (p x p matrix) covariance matrix
   - *mu* (p-dimensional vector) shift parameter

  :Output:
   - *xi* Density of SAL distribution at X
  """
  xi = np.empty(X.shape[0])
  d = distance.mahalanobis(X, mu, np.linalg.inv(cov))
  u = np.sqrt((2 + a.T @ np.linalg.inv(cov) @ a) * d)
  v = (2 - p) / 2
  print("u-value: ", u, "delta: ", d)

  for i in range(X.shape[0]):
    xi[i] = (2*np.exp((X - mu).T @ np.linalg.inv(cov) @ a)) / ((2*np.pi)**(p/2) * np.abs(np.linalg.norm(cov))**(1/2))
    xi[i] *= (d / (2 + a.T @ np.linalg.inv(cov) @ a))**(v/2)
    xi[i] *= kn(0, u)

  return xi

# Testing xi function used in expectation calculation
X = inp
p = X.shape[0]
a = np.random.rand(p, 1)
mu = np.random.rand(p, 1) 

# generate covariance matrix
temp = np.random.rand(p, p)
cov = np.dot(temp, temp.T) # guaranteed positive semi-definite


# Reshape alpha and mu


out = xi(p, X, a, cov, mu)
print("Output of xi function: " , out)

fig, axes = plt.subplots(figsize=(10, 5))
axes.plot(X[:, 0], xi(p, X, a, cov, mu))

plt.show()

# From page 2 of Franczak et al.
def expectation(a, b, v):
  """Calculate E[X] for a Generalized Inverse Gaussian (GIG)
  
  :Input:
   - *a* (float) positive real number
   - *b* (float) positive real number
   - *v* (float) real number

  :Output:
   - *E* (float) expectation of GIG
  """

  num = np.sqrt(b) * kn(v + 1, np.sqrt(a*b))
  denom = np.sqrt(a) * kn(v, np.sqrt(a*b))

  E = num / denom

  return E

def expectation_r(a, b, v):
  """Calculate E[1/X] for a Generalized Inverse Gaussian (GIG)
  
  :Input:
   - *a* (float) positive real number
   - *b* (float) positive real number
   - *v* (float) real number

  :Output:
   - *E* (float) reciprocal expectation of GIG
  """

  num = np.sqrt(a) * kn(v + 1, np.sqrt(a*b))
  denom = np.sqrt(b) * kn(v, np.sqrt(a*b))

  E = (num / denom) - (2*v/b)

  return E

# From page 4 of Franczak et al.
def SAL_expectation(X, t, p, cov, alpha, mu):
  """Calculate the expected value Q of the complete-data log-likelihood
     for a Shifted Asymmetric Laplace distribution

  :Input:
   - *X* (N x 1 vector) Observed data
   - *t* (N x G matrix) Component membership labels tau (each tau is a one-hot-encoded vector)
   - *p* (int) Dimensionality of SAL distribution
   - *cov* (G x G matrix) Covariance matrix
   - *alpha* (p x 1 vector) Skewness
   - *mu* (p x 1 vector) shift parameter

   :Output:
   - Q (float) Expectation
  """
  # Initialize variables
  G = t.shape[1]
  N = t.shape[0]
  pi = [1/G for i in range(G)]
  W = expon()
  v = 1 # random initialization for v
  
  # Define tau_g
  def t_g(x, a, cov, mu, g):
    num = pi[g] * xi(p, x, a[g], cov, mu[g])
    denom = 0
    for j in range(G):
      denom += pi[j] * xi(p, x, a[j], cov, mu[j])

    return num / denom

  # Define n_g
  def n_g(tau, g, N):
    n_g = 0
    for i in range(N):
      n_g += tau[i, g]

    return n_g
  
  # Let Q be broken up into 8 individual terms calculated below
  q1, q2, q3, q4, q5, q6, q7, q8 = [0 for i in range(8)]

  for g in range(G):
    q1 += n_g(t, g, N) * np.log(pi[g])
  
  q2 = (N*p*np.log(2*np.pi)) / 2

  for i in range(N):
    q3 += W.expect()
  q3 *= (N*p) / 2

  for g in range(G):
    q4 += (n_g(t, g, N)/2) * np.log(np.abs(np.linalg.inv(cov)))
  
  for i in range(N):
    for g in range(G):
      q5 += t_g(X[i], alpha, cov, mu, g) * (X[i] - mu[g]).T @ np.linalg.inv(cov) @ alpha[g]
  q5 *= 2

  for i in range(N):
    for g in range(G):
      q6 += t_g(X[i], alpha, cov, mu, g) * (X[i] - mu[g]).T @ (1/W[i]).expect() * np.linalg.inv(cov) @ (X[i] - mu[g])
  q6 *= (1/2)

  for i in range(N):
    for g in range(G):
      q7 += t_g(X[i], alpha, cov, mu, g) * W.expect() * alpha[g].T @ np.linalg.inv(cov) @ alpha[g]
  q7 *= (1/2)

  for i in range(N):
    for g in range(G):
      q8 += t_g(X[i], alpha, cov, mu, g) * W.expect()

  Q = q1 - q2 - q3 + q4 + q5 - q6 - q7 - q8
  
  return Q

# Testing Expectation function for SAL EM
p = 1
X = np.random.randint(1, 10, size=(p, 1))
a = np.random.randint(1, 10, size=(p, 1))
mu = np.random.rand(p, 1)

# generate covariance matrix
temp = np.random.randint(1, 100, size=(p, p))
cov = np.dot(temp, temp.T) # guaranteed positive semi-definite

# Create tau indicator matrix
tau = np.zeros((20, 10))
for i in range(tau.shape[1]):
  tau[i, i] = 1